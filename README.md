# Gemini API Pool

## Overview

This project provides a lightweight, high-performance proxy server written in Rust. It exposes an OpenAI-compatible API endpoint (`/v1/chat/completions`) and intelligently routes requests to the Google Gemini API. It manages a pool of Gemini API keys, rotating them for each request to distribute the load and manage API quotas effectively.

The entire application is containerized with Docker for easy, one-click deployment.

PS: The entire application is generated by Claude Code except this line, feel free to use it.

## Features

- **OpenAI API Compatibility**: Drop-in replacement for services that use the OpenAI Chat Completions API format.
- **API Key Rotation**: Automatically rotates Gemini API keys from a predefined pool on each request.
- **Dynamic Model Selection**: Uses the `model` field from the request payload to target different Gemini models (e.g., `gemini-1.5-flash`, `gemini-1.5-pro`).
- **High Performance**: Built with Rust and Axum for asynchronous, fast, and reliable performance.
- **Easy Deployment**: One-command deployment using Docker and a simple shell script.
- **Configurable Port**: Easily change the host port on which the service runs.

## Prerequisites

- [Docker](https://www.docker.com/get-started)
- [Git](https://git-scm.com/downloads)

## Getting Started

### 1. Clone the Repository

```bash
git clone <repository-url>
cd api-balance
```

### 2. Configuration

The application requires a `.env` file to store your Gemini API keys.

1.  **Create the `.env` file** by copying the provided example:
    ```bash
    cp gemini-pool/.env.example gemini-pool/.env
    ```

2.  **Edit `gemini-pool/.env`** and add your keys. The keys should be comma-separated.

    ```dotenv
    # gemini-pool/.env

    # Put your Gemini API keys here, separated by commas.
    GEMINI_API_KEYS=your_key_1,your_key_2,your_key_3

    # The address to bind the server to (inside the container).
    # This must be 0.0.0.0:8080 to be accessible from the host via Docker.
    LISTEN_ADDR=0.0.0.0:8080
    ```

### 3. Deployment with Docker (Recommended)

The included `deploy.sh` script handles everything from building the Docker image to running the container.

1.  **Make the script executable**:
    ```bash
    chmod +x deploy.sh
    ```

2.  **Run the script**:
    - To run on the default port `8080`:
      ```bash
      ./deploy.sh
      ```
    - To run on a custom port (e.g., `9000`):
      ```bash
      ./deploy.sh 9000
      ```

The script will build the image, stop any old containers, and start a new one in the background.

- **To view logs**: `docker logs -f gemini-pool-container`
- **To stop the service**: `docker stop gemini-pool-container`

## API Usage

Send a POST request to the `/v1/chat/completions` endpoint. The request body should be in the OpenAI Chat Completions format.

### Example Request

Here is an example using `curl` to interact with the service. You can specify any supported Gemini model in the `model` field.

```bash
curl -X POST http://127.0.0.1:8080/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
  "model": "gemini-1.5-flash",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "Hello! What is the capital of France?"
    }
  ]
}'
```

The server will forward this request to the Gemini API using one of your keys and return a response in the OpenAI format.

### List Models

To see the list of available models supported by this proxy, send a GET request to the `/v1/models` endpoint.

```bash
curl http://127.0.0.1:8080/v1/models
```
